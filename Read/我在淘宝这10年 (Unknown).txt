- Your Highlight on Location 131-136 | Added on Wednesday, June 18, 2014 11:16:39 PM

（本文所涉及的技术与数据均来源于互联网） 为了有一个更直观的对比，我们说一个同行，他在2011年光 棍节之前做促销，数据流量达到了12Gbps（他们有这么大的流 量，老板很高兴，在微博上公布了这个数据），这时的流量达到 了极限，网站几乎挂掉，用户无法下订单。而淘宝网光棍节当天 网络的流量最高达到800Gbps，带给各家银行和快递公司的流量 也让他们如临大敌（后来，他们以能够撑住淘宝带来的流量为荣 而到处宣传）。另外，如果你在网上购买过火车票，更能体会到 网站能支持多大的流量有多重要。但这不是一朝一夕就能做出来 的，也不是有钱就能办到的。
==========
- Your Highlight on Location 140-142 | Added on Wednesday, June 18, 2014 11:16:52 PM

如今淘宝网的流量已经是全球排名第12、国内排名第3（至 2011年年底，eBay全球排名第20，国内前两名是百度和腾讯）。 淘宝网的系统也从使用一台服务器，到采用万台以上的服务器。
==========
- Your Highlight on Location 155-159 | Added on Wednesday, June 18, 2014 11:20:19 PM

当时对整个项目组来说，压 力最大的就是时间，为什么时间这么重要呢？火云邪神先生说过 “天下武功无坚不破，唯快不破”，还有一个原因就是当时eBay 和易趣在资本方面正打得不可开交，我们是趁虚而入的，等他们 反应过来就危险了。那怎么在最短的时间内把一个网站从零开始 建立起来呢？了解淘宝历史的人都知道淘宝是在2003年5月10日 上线的，2003年4月7日到5月10日，这之间只有一个月时间。要 是你在这个团队里，你怎么做？不是“抄一个来”，我们的答案 是——“买一个来”。
==========
- Your Highlight on Location 162-164 | Added on Wednesday, June 18, 2014 11:20:47 PM

答案是：轻量一点的， 简单一点的。于是买了这样一个架构的网站：LAMP（Linux+ Apache+MySQL+PHP），这个直到现在还是一个很常用的网站架 构模型，其优点是：无须编译，发布快速，PHP语言功能强大， 能做从页面渲染到数据访问所有的事情，而且用到的技术都是开 源、免费的。
==========
- Your Highlight on Location 170-173 | Added on Wednesday, June 18, 2014 11:21:19 PM

其中最有技 术含量的是对数据库进行了一个修改，原来是从一个数据库进行 所有的读写操作，现在把它拆分成一个主库、两个从库，并且读 写分离。这么做的好处有几点：存储容量增加了，有了备份，使 得安全性增加了，读写分离使得读写效率得以提升（写要比读更 加消耗资源，分开后互不干扰）。这样整个系统的架构就如下图 所示。
==========
- Your Highlight on Location 178-182 | Added on Thursday, June 19, 2014 8:44:10 AM

（PHPAuction系统里只有拍卖的交易，Auction即拍卖的意 思。@_行癫在微博中提到：今天，eBay所有的交易中，拍卖交易 仍然占40%，而在中国，此种模式在淘宝几乎从一开始就未能占 据优势，如今在主流的交易中几乎可以忽略不计。背后的原因一 直令人费解，我大致可以给出其中一种解释，eBay基本上只在发 达国家展开业务，制造业外包后，电子商务的基本群体大多只能 表现为零散的个体间交易。）
==========
- Your Highlight on Location 190-200 | Added on Thursday, June 19, 2014 8:49:44 AM

这里有必要提一下当时的市场环境，非典（SARS）的肆 虐使得大家都不敢出门，尤其是去类似商场等人多的地方。 另外，在神州大地上最早出现的C2C网站易趣也正忙得不亦乐 乎，2002年3月，eBay以3000万美元收购了易趣公司33%的股 份，2003年6月以1.5亿美元收购了易趣公司剩余67%的股份。 当时，淘宝网允许买卖双方留下联系方式，允许同城交易，整 个操作过程简单轻松。而eBay是收费的，为了收取交易佣金， eBay禁止买卖双方这么做，这必然增加了交易过程的难度。而 且eBay为了全球统一，把易趣原来的系统替换成了美国eBay的 系统，用户体验一下全变了，操作起来非常麻烦，很多易趣的 卖家在那边都混不下去了，这等于是把积累的用户拱手送给了 淘宝。为了不引起eBay的注意，淘宝网在2003年里一直声称自 己是一个“个人网站”。由于这个创业团队强大的市场开拓和 运营能力，淘宝网的发展非常迅猛，2003年年底就吸引了注册 用户23万个，每日31万个PV，从2003年5月到同年年底成交额达 3371万元。这没有引起eBay的注意，却引起了阿里巴巴内部很 多员工的注意，他们觉得这个网站以后会成为阿里巴巴强劲的 对手，甚至有人在内网发帖，忠告管理层要警惕
==========
- Your Highlight on Location 209-212 | Added on Thursday, June 19, 2014 8:54:10 AM

公司给这几个人租了房 子，他们合住在湖畔花园旁边的小区里（男女分开），每天睁开 眼就去公司，半夜两三点收工睡觉，响应用户的需求非常快。反 过来对比一下，易趣被eBay收购之后，系统更换成了全球通用的 版本，响应用户的一个需求需要层层审批，据说，买个办公桌都 要走两个月流程，反应速度自然慢了下来。
==========
- Your Highlight on Location 212-214 | Added on Thursday, June 19, 2014 8:54:47 AM

当时淘宝第一个版本的系统中已经包含了商品发布、管理、 搜索、商品详情、出价购买、评价投诉、我的淘宝等功能（现 在主流程中也是这些模块。在2003年10月增加了一个功能节点： “安全交易”，这是支付宝的雏形）。
==========
- Your Highlight on Location 212-217 | Added on Thursday, June 19, 2014 8:55:13 AM

当时淘宝第一个版本的系统中已经包含了商品发布、管理、 搜索、商品详情、出价购买、评价投诉、我的淘宝等功能（现 在主流程中也是这些模块。在2003年10月增加了一个功能节点： “安全交易”，这是支付宝的雏形）。随着用户需求和流量的不 断增长，系统做了很多日常改进，服务器由最初的一台变成了三 台，一台负责发送Email、一台负责运行数据库、一台负责运行 WebApp。一段时间之后，商品搜索的功能占用数据库资源太大了 （用like搜索的，很慢），2003年7月，多隆又把阿里巴巴中文站 的搜索引擎iSearch搬了过来。
==========
- Your Highlight on Location 221-225 | Added on Thursday, June 19, 2014 8:56:02 AM

随着访问量和数据量的飞速上涨，问题很快就出来了，第一 个问题出现在数据库上。MySQL当时是第4版的，我们用的是默 认的存储引擎MyISAM，这种存储引擎在写数据的时候会把表锁 住。当Master同步数据到Slave的时候，会引起Slave写，这样在Slave 的读操作都要等待。还有一点是会发生Slave上的主键冲突，经常会 导致同步停止，这样，你发布的一些东西明明已经成功了，但就是 查询不到。另外，当年的MySQL不比如今的MySQL，在数据的容 量和安全性方面也有很多先天的不足（和Oracle相比）。
==========
- Your Highlight on Location 226-231 | Added on Thursday, June 19, 2014 8:57:53 AM

讲到这里，顺便先辟个谣，网上有很多这样骗转发的励志段 子：“1998年，马化腾等一伙人凑了50万元创办了腾讯，没买 房；1998年，史玉柱借了50万元搞脑白金，没买房；1999年，丁 磊用50万元创办了163.com，没买房；1999年，陈天桥炒股赚了50 万元，创办盛大，没买房；1999年，马云等18人凑了50万元注册 了阿里巴巴，没买房。如果当年他们用这50万元买了房，现在估 马云自己的房子里创办的，阿里巴巴是1999年初发布上线的。所 以，关于马云买房子的事情，真相是这样的。
==========
- Your Highlight on Location 232-237 | Added on Thursday, June 19, 2014 8:58:14 AM

马云在2003年7月就宣布这个是阿里巴巴旗下的网站， 随后在市场上展开了很成功的推广运作。最著名的就是利用中小 网站来做广告，突围eBay在门户网站上对淘宝的广告封锁。这时 候，eBay终于看到淘宝网这个后起之秀了，他对竞争者的态度就 是“封杀他”。eBay买断了新浪、搜狐、网易的电子商务类型的 广告，签署了排他性协议，切断了淘宝在这上面做广告的路子。 大路不通，我们就独辟蹊径，上网比较早的人应该还记得那些在 右下角的弹窗和网站腰封上一闪一闪的广告，“淘宝网”几个字 总是如影随形地出现在任何中小型网站上。市场部那位到处花钱 买广告的家伙太能花钱了，一出手就是几百万元，他被我们称为 “大少爷”。
==========
- Your Highlight on Location 237-244 | Added on Thursday, June 19, 2014 8:58:47 AM

“大少爷”们做的广告，带来的就是迅速上涨的流量和交易 量。在2003年年底，MySQL已经撑不住了，技术的替代方案非常 简单，就是换成Oracle。换为Oracle的原因除了它容量大、稳定、 安全、性能高之外，还有人才方面的原因。在2003年的时候，阿 里巴巴已经有一支很强大的DBA团队了，有鲁国良、冯春培、 汪海（七公）这样的人物，后来还有冯大辉（@fenng）、陈吉 平（拖雷）。这样的人物牛到什么程度呢？Oracle给全球的技术 专家颁发一些头衔，其中最高级别的叫ACE（就是扑克牌的“尖 儿”，够大的吧），被授予这个头衔的人目前全球也只有300多名 （公布名单的网址为： http://apex.oracle.com/pls/otn/f?p=19297:3 ），当年全球只有十几名，而阿里巴巴就有4名。有如此强大的技 术后盾，把MySQL换成Oracle是顺理成章的事情。
==========
- Your Highlight on Location 245-254 | Added on Thursday, June 19, 2014 8:59:50 AM

Oracle的性能和并发访问能力 之所以如此强大，有一个关键性的设计——连接池，连接池中放 的是长连接，是进程级别的，在创建进程的时候，它就要独占一 部分内存空间。也就是说，这些连接数在固定内存的Oracle Server 上是有限的，任何一个请求只需要从连接池中取得一个连接即 可，用完后释放，这不需要频繁地创建和断开连接，而连接的创 建和断开的开销是非常大的。但对于PHP语言来说，它对数据库 的访问都是很直接的，每一个请求都要一个连接。如果是长连 接，应用服务器增多时，连接数就多了，就会把数据库拖挂，如 果是短连接，频繁地连接后再断开，性能会非常差（而Java语言有 很多现成的连接池）。那如何是好呢？我们打探到eBay用了一个 连接池的工具，是BEA卖给他们的。我们知道，BEA的东西都很 贵，我们买不起，就放弃了找BEA的念头，于是多隆在网上寻寻 觅觅，找到一个开源的连接池代理服务SQL Relay（http://sqlrelay. sourceforge.net），这个东西能够提供连接池的功能，多隆对它进 行了一些功能改进之后，系统的架构就变成了如下形式。
==========
- Your Highlight on Location 261-268 | Added on Thursday, June 19, 2014 9:01:29 AM

替换完数据库后，时间到了2004年春天，俗话说“春宵一刻 值千金”，但这些人的春宵却不太好过，他们在把数据的连接放 在SQL Relay之后就噩梦不断，这个代理服务经常会死锁，如同之 前的MySQL死锁一样。虽然多隆做了很多修改，但当时那个版本 内部处理的逻辑不对，问题很多，最快的解决办法就是“重启” 它的服务。这在白天还好，只要连接上机房的服务器，把进程杀 掉，然后开启就可以了。但是最痛苦的是它在晚上也要死掉，于 是工程师们不得不24小时开着手机，一旦收到“SQL Relay进程挂 起”的短信，就从春梦中醒来，打开电脑，连上机房的网络，重 启服务，后来干脆每天睡觉之前先重启一下。做这事最多的据说 是三丰，他现在是淘宝网的总裁。现在我们知道，任何牛B的人 物，都有一段苦B的经历。
==========
- Your Highlight on Location 268-278 | Added on Thursday, June 19, 2014 9:02:26 AM

微博上有人说“好的架构是进化来的，不是设计来的”。的 确如此，其实还可以再加上一句“好的功能也是进化来的，不是 设计来的”。在架构的进化过程中，业务的进化也非常迅猛。最 早的时候，买家打钱给卖家都是通过银行转账汇款，有些骗子收 了钱却不发货，干脆逃之夭夭。这是一个很严重的问题，一个人 这么干了之后，很快就有更多的人学会了（这就是传说中的“病 毒传播”）。然而魔高一尺，道高一丈，淘宝网这伙人开始研 究防骗子的解决方案，他们看了PayPal的支付方式，发现不能解 决问题。研究了类似QQ币的东西，想弄个“淘宝币”出来，发 到了“担保交易”这种第三方托管资金的办法。于是在2003年10 月，淘宝网上线了一个功能，叫做“安全交易”，卖家如果选择 支持这种功能，买家就会把钱交给淘宝网，等他收到货之后，淘 宝网再把钱给卖家，这就是现在的“支付宝”。这个功能最早是 让卖家可选的，因为这会延迟他收款的周期。但一旦卖家用了这 个之后，就发现交易量猛增，一年之后，几乎所有的卖家都选择 担保交易，到后来干脆所有的交易都必须走担保交易。在2012年 支付宝的年会上，支付宝公布2011年的交易笔数已经是PayPal的两 倍。这个划时代的创新，其实就是在不断思索过程中的一个灵光 乍现。
==========
- Your Highlight on Location 278-285 | Added on Thursday, June 19, 2014 9:04:08 AM

当时开发“安全交易”功能的是茅十八和他的徒弟苗人凤 （茅十八开发到一半去上海读MBA了，苗人凤现在是支付宝的 首席业务架构师），开发与银行网关对接功能的是多隆。当时多 数银行的网站已经支持在线支付了，但多隆告诉我，他们的网关 五花八门，用什么技术的都有，我们必须一家一家地去接。而且 银行的网关不保证用户付钱后就一定能扣款成功，不保证扣款成 功后就一定通知淘宝，也不保证通知淘宝后就一定能通知到，以 及不保证通知到了就不重复通知。这害苦了苗人凤，他必须每天 手工核对账单，少一分钱都睡不着觉，因为对不齐账就一定是有 人的钱找不到地方了，这可是天大的问题。另外，他为了测试这 些功能，去杭州所有的银行都办理了一张银行卡。一大堆银行卡 实里面都只是十元八元的。现在我们再一次知道，任何牛B的人 物，都必须有一段苦B的经历。
==========
- Your Highlight on Location 322-325 | Added on Thursday, June 19, 2014 9:10:15 AM

我们的答案是请Sun公司的人。没错，就是创造Java语言的那 家公司，世界上没有比他们更懂Java的了。除此之外，还有一个不 为人知的原因，我刚才说到Java被世界上主流的大规模网站普遍采 用，其中有一个网站就是eBay，那时侯eBay的系统刚刚从C++ 改 到Java，而且就是请Sun的工程师给改造成Java架构的，这下你懂 了吧？他们不仅更懂Java，而且更懂eBay。
==========
- Your Highlight on Location 326-330 | Added on Thursday, June 19, 2014 9:11:13 AM

现在摆在他 们面前的问题是用什么办法把一个庞大的网站从PHP语言迁移到 Java？而且要求在迁移的过程中，不停止服务，原来系统的bugfix 和功能改进不受影响。亲，你要是架构师，你怎么做？有人的答 案是写一个翻译器，如同把中文翻译成英文一样，自动翻译。我 只能说你这个想法太超前了，“too young, too simple, sometimes naive”。当时没有，现在也没有人能做到。他们的大致方案是 给业务分模块，一个模块一个模块地渐进式替换。
==========
- Your Highlight on Location 337-347 | Added on Thursday, June 19, 2014 9:13:15 AM

说了开发模式，再说说用到的Java MVC框架，当时的struts1.x 是用得比较多的框架，但是用过webwork和struts2的人可能知道， struts1.x在多人协作方面有很多致命的弱点，由于没有一个轻量框 架作为基础，因此，很难扩展，这样架构师对于基础功能和全局 功能的控制就很难做到。而阿里巴巴的18个创始人之中，有个架 构师周悦虹，他在Jakarta Turbine的基础上做了很多扩展，打造了 一个阿里巴巴自己用的MVC框架WebX （ http://www.openwebx. org/docs/Webx3_Guide_Book.html ），这个框架易于扩展，方 便组件化开发，它的页面模板支持JSP和Velocity等，持久层支持 ibatis和hibernate等，控制层可以用EJB和Spring（Spring是后来才有 的）。项目组选择了这个强大的框架。另外，当时Sun在全世界大 力推广他们的EJB，虽然淘宝的架构师认为这个东西用不到，但他 们还是极力坚持。在经历了很多次的技术讨论、争论甚至争吵之 后，这个系统的架构就变成了下图的形式。 MVC框架是阿里的WebX，控制层用了EJB，持久层是ibatis。 另外，为了缓解数据库的压力，商品查询和店铺查询放在搜索引 擎中。这个架构图是不是好看了一点了？
==========
- Your Highlight on Location 351-357 | Added on Thursday, June 19, 2014 9:15:57 AM

iSearch 其实是在LAMP系统运行一段时间之后被多隆引进的，换为Oracle 之后只是替换一下数据源）。其实这个搜索引擎的原理很简单， 就是把数据库里的数据dump（倾倒）成结构化的文本文件后，放 在硬盘上，提供Web应用以约定的参数和语法来查询这些数据。 这看起来不难，难的是数以亿计的信息，怎么做到快速更新呢？ 这好比你做了一个网站，在百度上很快就能搜到，你一定很满意 了。但如果你发布一件商品，在淘宝上过1个小时还搜不到，你肯 定要郁闷了。另一个难点是如何保证非常高的容量和并发量？再 往后面就要考虑断句和语义分析的问题，以及推荐算法等更加智 能的问题。这些内容先不详细介绍，因为搜索引擎的技术已经足 以写好几本书了
==========
- Your Highlight on Location 365-371 | Added on Thursday, June 19, 2014 9:18:20 AM

到现在为止，我们已经用上了IBM的小型机、Oracle的数据 库、EMC的存储，这些东西都是很贵的，那些年可以说是花钱 如流水。有人说过“钱能解决的问题，就不是问题”，但随着淘 宝网的发展，在不久以后，钱已经解决不了我们的问题了。花钱 买豪华的配置，也许能支持1亿个PV的网站，但淘宝网的发展实 在是太快了，到了10亿个PV怎么办？到了百亿怎么办？在几年以 后，我们不得不创造技术，解决这些只有世界顶尖的网站才会遇 步地把IOE（IBM小型机、Oracle、EMC存储）这几个“神器” 都去掉了。这些神器就如同《西游记》中那些神仙的兵器，他们 身边的妖怪们拿到这些兵器能把猴子打得落荒而逃。但最牛的神 仙是不依赖这些神器的，他们挥一挥衣袖、翻一下手掌就威力无 比了。
==========
- Your Highlight on Location 383-389 | Added on Thursday, June 19, 2014 9:25:28 AM

如果我是卖家， 查看我的商品没有问题，我们都在一个库里。但如果我是一个 买家，买的商品有DB1的，也有DB2的，要查看“我已买到的宝 贝”的时候，应用程序怎么办？必须到两个数据库中分别查询对 应的商品。要按时间排序怎么办？两个库中“我已买到的宝贝” 全部查出来在应用程序中做合并。另外，分页怎么处理？关键字 查询怎么处理？专业点的说法就是数据的Join没法做了。这些工 数据库路由的框架DBRoute，统一处理了数据的合并、排序、分 页等操作，让程序员像使用一个数据库一样操作多个数据库里的 数据，这个框架在淘宝的Oracle时代一直在使用。但是后来随着 业务的发展，这种分库的第二个目的——“容灾”的效果没有达 到。像评价、投诉、举报、收藏、我的淘宝等很多地方，都必须 同时连接DB1和DB2，哪个库挂了都会导致整个网站挂掉。
==========
- Your Highlight on Location 390-392 | Added on Thursday, June 19, 2014 9:25:36 AM

上一篇说过，采用EJB其实是和Sun的工程师妥协的结果，在 他们离开之后，EJB也逐渐被冷落了下来。在2005年和2006年的时 候，Spring大放异彩，于是在控制层用Spring替换掉了EJB，给整 个系统精简了很多代码。
==========
- Your Highlight on Location 394-405 | Added on Thursday, June 19, 2014 9:29:13 AM

到了2005年，商 品数有1663万个，PV有8931万个，注册会员有1390万个，这给数 据存储带来的压力依然很大，数据量大，速度就慢。亲，除了搜 索引擎、分库分表，还有什么办法能提升系统的性能？一定还有 招数可以用，这就是缓存和CDN（内容分发网络）。 你可以想象，9000万次的访问量，有多少是在商品详情页 面？访问这个页面的时候，数据全都是只读的（全部从数据库中 读出来，不写入数据库），在那个时候，我们的架构师多隆大神 做了一个基于 Berkeley DB 的缓存系统，把很多不太变动的只读 信息放了进去。其实最初这个缓存系统还比较弱，我们并不敢把 所有能缓存的信息都往里面放，一开始先把卖家的信息放里面， 然后把商品属性放里面，再把店铺信息放里面，但是像商品详情 这类字段太大的放进去受不了。说到商品详情，这个字段比较恐 怖，有人统计过，淘宝商品详情打印出来平均有5米长，在系统里 其实放在哪里都不招人待见。笔者清楚地记得，我来淘宝之后担 任项目经理做的第一个项目就是把商品详情从商品表中移出来。 它最早与商品的价格、运费等信息放在一个表中，拖慢了整张表 的查询速度，而很多时候查询商品信息是不需要查看详情的。于 是在2005年的时候，我把商品详情放在数据库的另外一张表中， 再往后，这个大字段被从数据库中请了出来，先是放入了缓存系 统，到现在是放进了文件系统TFS中。
==========
- Your Highlight on Location 405-411 | Added on Thursday, June 19, 2014 9:29:44 AM

到现在为止，整个商品详情的页面都在缓存里面了，眼尖的 读者可能会发现现在的商品详情不全是“只读”的信息了，这个 页面上有个信息叫“浏览量”（这个信息是2006年加上去的）， 这个数字每刷新一次，页面就要“写入”存储一次，这种高频度 实时更新的数据能用缓存吗？通常来说，这种是必须放进数据库 的，但是悲剧的是，我们在2006年开发这个功能的时候，把浏览 量写入数据库，发布上线1个小时后，数据库就挂掉了，每天几亿 次的写入，数据库承受不了。那怎么办？亲，……先不回答你， 后面讲到缓存Tair的时候再说。（下图不是广告，请把注意力从 左边移到右边中间，看看浏览量这个数据在哪里。）
==========
- Your Highlight on Location 414-419 | Added on Thursday, June 19, 2014 9:33:33 AM

我说过淘宝的CDN系统支撑了800Gbps以 上的流量，作为对比，我们可以看一下国内专业做CDN的上市公 司ChinaCache的介绍——“ChinaCache是中国第一的专业CDN服 务提供商，向客户提供全方位网络内容快速分布解决方案。作为 首家获信产部许可的CDN服务提供商，目前ChinaCache在全国50 多个大中城市拥有近300个节点，全网处理能力超过500Gbps，其 CDN网络覆盖中国电信、中国网通、中国移动、中国联通、中国 铁通和中国教育科研网等各大运营商。”——淘宝一家的流量比 他们的加起来还要多，这样你可以看出淘宝在CDN上的实力，这 在全世界都是数一数二的（其实我们一开始用的商用CDN就是 ChinaCache，它们支撑了很长时间）
==========
- Your Highlight on Location 419-422 | Added on Thursday, June 19, 2014 9:33:45 AM

另外，因为CDN需要大量 的服务器，要消耗很多能源（消耗多少？在前两年我们算过一笔 账，淘宝上产生一个交易，消耗的电量足以煮熟4个鸡蛋）。这两 年，章文嵩的团队又在研究低功耗的服务器，在绿色计算领域也 做了很多开创性的工作，我们定制的基于英特尔凌动处理器的低 功耗服务器已经部署到了CDN机房，降低了很大的能耗。
==========
- Your Highlight on Location 423-427 | Added on Thursday, June 19, 2014 9:34:01 AM

回想起刚用缓存那段时间，笔者还是个菜鸟，有一个经典的 错误常常犯，就是更新数据库的内容时，忘记通知缓存系统，结 果在测试的时候就发现我改过的数据在页面上没有变化。后来做 了一些页面上的代码，修改CSS和JS的时候，用户本地缓存的信息 没有更新，页面上也会乱掉，在论坛上被人说的时候，我告诉他 用Ctrl+F5组合键（清除本地缓存刷新页面），然后赶紧修改脚本 文件的名称，重新发布页面。
==========
- Your Highlight on Location 427-428 | Added on Thursday, June 19, 2014 9:34:31 AM

我们对数据分库、放弃EJB、引入Spring、加入缓存、加入 CDN等工作，看起来没有章法可循，其实都是围绕着提高容量、 提高性能、节约成本来做的
==========
- Your Highlight on Location 495-499 | Added on Thursday, June 19, 2014 10:13:10 AM

历史总是惊人的巧合，在我们准备研发文件存储系统的时 候，Google走在了前面，2007年，他们公布了GFS（Google File System）的设计论文，这给我们带来了很多借鉴的思路。随后我 们开发出了适合淘宝使用的图片存储系统（TaoBao File System， TFS）。3年之后，我们发现历史的巧合比我们想象的还要神奇， 几乎跟我们同时，中国的另外一家互联网公司也开发了他们的文 件存储系统，甚至取的名字都一样——TFS，太神奇了
==========
- Your Highlight on Location 507-513 | Added on Thursday, June 19, 2014 10:15:39 AM

从上面的架构图可看出：集群由一对Name Server和多台Data Server构成，Name Server的两台服务器互为双机，这就是集群文 件系统中管理节点的概念。 在这个系统中： y每个Data Server运行在一台普通的Linux主机上； y以Block文件的形式存放数据文件（一个Block的大小一般为 64MB）； yBlock存储多份是为了保证数据安全； y利用ext3文件系统存放数据文件； y磁盘raid5做数据冗余； y文件名内置元数据信息，用户自己保存TFS文件名与实际文 件的对照关系，这使得元数据量特别小。
==========
- Your Highlight on Location 513-520 | Added on Thursday, June 19, 2014 10:17:07 AM

淘宝TFS文件系统在核心设计上最大的取巧在于，传统的集群 系统中元数据只有1份，通常由管理节点来管理，因而很容易成为 瓶颈。而对于淘宝网的用户来说，图片文件究竟用什么名字来保 存他们并不关心，因此，TFS在设计规划上考虑在图片的保存文 件名上暗藏了 一些元数据信息，例如，图片的大小、时间、访问 频次等信息，包括所在的逻辑块号。而在实际的元数据上，保存 的信息很少，因此，元数据结构非常简单。仅仅只需要一个FileID 就能够准确定位文件在什么地方。由于大量的文件信息都隐藏在 文件名中，整个系统完全抛弃了传统的目录树结构，因为目录树 开销最大。拿掉后，整个集群的高可扩展性可极大地提高。实际 上，这一设计理念和目前业界的“对象存储”较类似。
==========
- Your Highlight on Location 532-534 | Added on Thursday, June 19, 2014 10:18:28 AM

在TFS 1.3版本中，工程师们重点改善了心跳和同步的性能， 最新版本的心跳和同步在几秒钟之内就可完成切换，同时进行了 一些新的优化，包括元数据存储在内存中、清理磁盘空间等。
==========
- Your Highlight on Location 543-547 | Added on Thursday, June 19, 2014 10:26:34 AM

值得一提的是，根据淘宝网的缩略图生成规则，缩略图都是 实时生成的。这样做的好处有两点：一是为了避免后端图片服务 器上存储的图片数量过多，大大节约后台存储空间的需求，我们 计算过，采用实时生成缩略图的模式比提前全部生成好缩略图的 模式节约90%的存储空间。也就是说，存储空间只需要后一种模 式的10%。二是，缩略图可根据需要实时生成，这样更加灵活。
==========
- Your Highlight on Location 559-560 | Added on Thursday, June 19, 2014 10:27:13 AM

目前淘宝网的TFS已经开源（见code.taobao.org），业界的同 仁可以一起使用和完善这个系统。
==========
- Your Highlight on Location 573-579 | Added on Thursday, June 19, 2014 10:30:09 AM

用户操作也不方便，如果一个 人有很多商品，上下架需要一个一个地操作，非常麻烦（想想那 些卖书的）。这时候一个重要人物承志（现在的蘑菇街CEO） 登场了，他给我们演示了最牛的前端交互技术，就是Gmail上那 种AJAX的交互方式，可以拖动，可以用鼠标右键，也可以用组 合键，操作完毕还不刷新页面，管理商品如有神助，帅呆了。我 伙工程师就开始行动了。我们热火朝天地干了三个月，快要完成 的时候，老马突然出现在我身后，看我操作了一遍新版“我的淘 宝”之后，问我这是不是客户端软件，我说是网页，他抓狂了， 说这跟客户端软件一样，链接下面的下画线都没有，上下架用文 件夹表示，他都不知道怎么操作，卖家肯定也不会玩。
==========
- Your Highlight on Location 582-590 | Added on Thursday, June 19, 2014 10:31:07 AM

老马果然是神一样的人物，他说的应验了，淘宝历史上第一 个群体性事件爆发了，试用完新版本的“我的淘宝”之后，很多 卖家愤怒了，说不会玩儿。一灯就和承志一起商量怎么把页面改 得像网页一点，改了半个月，愤怒依然没有平息。我很无奈地看 着这两个人在那里坚持，然后跟老板们商量怎么办。后来我们到 论坛上让大家投票要不要使用新版“我的淘宝”，投票结果是一 半以上的人反对。于是这十来个人做了3个月的系统被杀掉了。 这让我非常沮丧，但最痛苦的还不是这个，我们下线之后，另外 一拨卖家不满了，说这么好的功能怎么没有了？这个产品带给我 们的是新技术（AJAX、prototype框架）的尝试，以及新技术对 用户操作习惯的改变，一定要慎之又慎。另外，还有一点没有总 结好的教训，就是应对群体事件的时候，我们手足无措，在后来 “招财进宝”和淘宝商城出现群体性事件的时候，我发现悲剧在 重演。
==========
- Your Highlight on Location 606-609 | Added on Thursday, June 19, 2014 10:38:04 AM

一般的缓存策略是不支持实时更新 的，这时候多隆大神想了个办法，在Pache上面写了一个模块，这 个数字根本不经过下层的WebApp容器（只经过Apache）就写入一 个集中式的缓存区了，这个缓存区的数据再异步更新到数据库。 这就是我前面提到的，整个商品详情的页面都在缓存中了，把缓 存用到了极致。
==========
- Your Highlight on Location 610-619 | Added on Thursday, June 19, 2014 10:39:09 AM

接下来，我们就说说缓存的技术吧。 淘宝在很早就开始使用缓存技术了，在2004年的时候，我们 使用一个叫做ESI（Edge Side Includes）的缓存（Cache）。在决 定采用ESI之前，多隆试用了Java的很多Cache，但都比较重，后 来用了Oracle Web Cache，也经常挂掉，Oracle Web Cache也支持 ESI，多隆由此发现了ESI这个好东东。ESI是一种数据缓冲/缓存 服务器，它提供将Web网页的部分（这里指页面的片段）进行缓 冲/缓存的技术及服务。以往的数据缓冲服务器和信息传送服务以 “页”为单位制作，复制到数据缓冲服务器中，这用于处理静态 页面很有效，但在面对动态内容的时候，就很难得到高效率。在 ESI中是部分的缓冲网页，使用基于XML的标记语言，指定想要 缓冲的页面部分。由此，页面内分为动态地变更部分和静态的不 变更部分，只将静态的部分有效地发送到服务器中。淘宝网的数 据虽然大部分都是动态产生的，但页面中的静态片段也有很多， 例如，页面的头、尾，商品详情页面的卖家信息等（如下图右 侧），这些最早都是从ESI缓存中读取的。
==========
- Your Highlight on Location 628-630 | Added on Thursday, June 19, 2014 11:04:07 AM

TBstore的分布式算法实现：根据保存的Key（关键字），对 key进行Hash算法，取得Hash值，再对Hash值与总Cache服务器数 据取模。然后根据取模后的值，找到服务器列表中下标为此值的 Cache服务器。由Java Client API封装实现，应用无须关心。
==========
- Your Highlight on Location 649-650 | Added on Thursday, June 19, 2014 11:08:43 AM

目前，Tair支撑了淘宝几乎所有系统的缓存信息。Tair已开 源，地址为code.taobao.org。
==========
- Your Highlight on Location 658-660 | Added on Thursday, June 19, 2014 11:19:30 AM

所有的进步都是不稳定的，一个问题解决了，我们不得不面对又一个新问 题。” ——马丁·路德·金
==========
- Your Highlight on Location 662-663 | Added on Thursday, June 19, 2014 11:25:24 AM

在系统发展的过程中，架构师的眼光至关重要，作为程序 员，只要把功能实现即可，但作为架构师，要考虑系统的扩展 性、重用性，对于这种敏锐的感觉，有人说是一种“代码洁 癖”。
==========
- Your Highlight on Location 741-748 | Added on Thursday, June 19, 2014 7:18:48 PM

系统这么拆分的好处显而易见，拆分之后的每个系统可以单 独部署，业务简单，方便扩容；有大量可重用的模块便于开发 新的业务；能够做到专人专事，让技术人员更加专注于某一个领 域。这样要解决的问题也很明显，分拆之后，系统之间还是必须 要打交道的，越往底层的系统，调用它的客户越多，这就要求底 层的系统必须具有超大规模的容量和非常高的可用性。另外，拆 分之后的系统如何通信？这里需要两种中间件系统，一种是实时 调用的中间件（淘宝的HSF，高性能服务框架），一种是异步消 息通知的中间件（淘宝的Notify）。另外，一个需要解决的问题 是用户在A系统登录后，到B系统的时候，用户的登录信息怎么保 存？这又涉及一个Session框架。再者，还有一个软件工程方面的 问题，这么多层的一套系统，怎么去测试它？
==========
- Your Highlight on Location 773-784 | Added on Thursday, June 19, 2014 7:22:56 PM

y 分工：收银员和打扫卫生的人分开，这种分工容易解决， 而这种分工在互联网中是一项重要而复杂的技术，没有现 实生活中这么简单，涉及的主要有按功能和数据库的不同 拆分系统等，如何拆分以及拆分后如何交互是需要面临的 两个挑战。因此，会有高性能通信框架、SOA平台、消息 中间件、分布式数据层等基础产品的诞生。 y 负载均衡：让每个收银台排队差不多长，设立小件通道、 团购通道、VIP通道等，这些可以认为都是集群带来的负载 均衡的问题，从技术层面上说，实现起来自然比生活中复 杂很多。 y 根据QoS分配资源：部分员工仅在晚上上班的机制要在现实 生活中做到不难，而对互联网应用而言，就是一件复杂而 且极具挑战的事。 参照生活中的例子来说，在面对用户增长的情况下，想出这 些招应该不难，不过要掌握以上四点涉及的技术就相当复杂了， 而且互联网中涉及的其他很多技术还没在这个例子中展现出来， 例如缓存、CDN等优化手段；运转状况监测、功能降级、资源劣 化、流控等可用性手段，自建机房、硬件组装等成本控制手段。 因此，构建一个互联网网站确实是不容易的，技术含量十足，当 然，经营一家超市也不简单。
==========
- Your Highlight on Location 784-795 | Added on Thursday, June 19, 2014 7:24:39 PM

从超市的运维可以抽象出系统设计的一些思路，服务拆分之 后，如何取得我需要的服务？在“电视机”上，把每个集群能提 供的服务显示出来。你不需要关心哪个人为你服务，当你有需要 的时候，请先看头顶的电视机，它告诉你哪个服务在哪个区域。 当你直接去这个区域的时候，系统会给你找到一个最快速的服务 通道。 这就是HSF的设计思想，服务的提供者启动时通过HSF框架 向ConfigServer（类似超市的电视机）注册服务信息（接口、版 本、超时时间、序列化方式等），这样ConfigServer上面就定义 了所有可供调用的服务（同一个服务也可能有不同的版本）；服 务调用者启动的时候向ConfigServer注册对哪些服务感兴趣（接 口、版本），当服务提供者的信息变化时，ConfigServer向相应 的感兴趣的服务调用者推送新的服务信息列表；调用者在调用时 则根据服务信息的列表直接访问相应的服务提供者，而无须经过 ConfigServer。我们注意到ConfigServer并不会把服务提供者的IP地 址推送给服务的调用者，HSF框架会根据负载状况来选择具体的 服务器，返回结果给调用者，这不仅统一了服务调用的方式，也 实现了“软负载均衡”。平时ConfigServer通过和服务提供者的心 跳来感应服务提供者的存活状态。
==========
- Your Highlight on Location 803-811 | Added on Thursday, June 19, 2014 7:46:46 PM

从上图HSF的标志来看，它的速度是很快的。HSF是一个分 布式的标准Service方式的RPC（Remote Procedure Call Protocol， 远程过程调用协议）框架，Service的定义基于OSGI的方式，通 讯层采用TCP/IP协议。关于分布式的服务框架的理论基础，HSF 的作者毕玄写了一篇博文（http://www.blogjava.net/BlueDavy/ archive/2008/01/24/177533.html），有关基于OSGI的分布式服 务框架，也有一系列的博文（http://www.blogjava.net/BlueDavy/ archive/2008/01/14/175054.html）。 从下面这个HSF监控系统的截图中可以更直观地看到一些信 息，在两个集群中有两个服务器（其实有更多的，没有全部截 图下来）都提供com.taobao.item.service.SpuGroupService 这一服 务，版本号都是1.0.0，这个服务在ConfigServer上的注册信息中 包含超时时间、序列化方式。在后面那条信息中可看到，在展开 的这个集群中服务有835台机器已订阅，这些订阅者有淘宝的服 务器（cart是购物车功能的服务器），也有hitao（淘花网）的服 务器。
==========
- Your Highlight on Location 812-820 | Added on Thursday, June 19, 2014 7:47:08 PM

HSF系统目前每天承担了300亿次以上的服务调用。 一些读者可能会有一个疑问：既然淘宝的服务化是渐进式 的，那么在HSF出现之前，系统之间的调用采用什么方式呢？ 这个有点“五花八门”，例如，对于类目的调用方式是： Forest打包成一个JAR包，在应用启动的时候装载到内存中，仅 这一个JAR包所占用的内存就有800MB之多（因为淘宝的类目数 据太庞大了），对于当时一般只有2GB内存的开发机来说，加载 完类目信息后，机器运行速度就非常慢。对于用户信息（UIC） 来说，一开始的调用方式是用Hessian接口。还有一些系统是通过 WebService、Socket甚至是HTTP请求来相互调用的。每种调用方 式都涉及各种超时、信息的加解/密、参数的定义等问题，由此可 见，在没有HSF之前，系统之间的调用是错综复杂的。而随着系 统拆分得越来越多，必须由一个统一的中间层来处理这种问题， HSF正是在这种背景下诞生的。
==========
- Your Highlight on Location 820-829 | Added on Thursday, June 19, 2014 7:48:20 PM

Notify HSF解决了服务调用的问题，我们再提出一个很早就说过的 问题：用户在银行的网关付钱后，银行需要通知到支付宝，但银 行的系统不一定能发出通知；如果通知发出了，不一定能通知 到；如果通知到了，不一定不重复通知一遍。这个状况在支付宝 持续了很长时间，非常痛苦。支付宝从淘宝剥离出来的时候，淘 宝和支付宝之间的通信也面临同样的问题，那是2005年的事情， 支付宝的架构师鲁肃提出用MQ（Message Queue）的方式来解决 这个问题，我负责淘宝这边读取消息的模块。但我们发现消息数 量上来之后，常常造成拥堵，消息的顺序也会出错，在系统挂掉 的时候，消息也会丢掉，这样非常不保险。然后鲁肃提出做一个 系统框架上的解决方案，把要发出的通知存放到数据库中，如果 实时发送失败，再用一个时间程序来周期性地发送这些通知，系 统记录下消息的中间状态和时间戳，这样保证消息一定能发出， 也一定能通知到，且通知带有时间顺序，这些通知甚至可以实现 事务性的操作。
==========
- Your Highlight on Location 830-836 | Added on Thursday, June 19, 2014 7:49:17 PM

例如，拍下一 件商品，在交易管理系统中完成时，它需要通知商品管理系统减 少库存，通知旺旺服务系统发送旺旺提醒，通知物流系统上门取 货，通知SNS系统分享订单，通知公安局的系统这是骗子……用 户的一次请求，在底层系统可能产生10次的消息通知。这一大堆 的通知信息是异步调用的（如果同步，系统耦合在一起就达不到 拆分的目的），这些消息通知需要一个强大的系统提供支持，从 消息的数量级上看，比支付宝和淘宝之间的消息量又上了一个层 次，于是按照类似的思路，一个更加强大的消息中间件系统就诞 生了，它的名字叫做Notify。Notify是一个分布式的消息中间件系 统，支持消息的订阅、发送和消费，其架构图如下所示。
==========
- Your Highlight on Location 852-855 | Added on Friday, June 20, 2014 12:36:12 AM

这时候，数据查询的 中间件就要能够承担这个重任了，它对上层来说，必须像查询一 个数据库一样来查询数据，还要像查询一个数据库一样快（每条 查询在几毫秒内完成），TDDL就承担了这样一个工作。 另外，加上数据的备份、复制、主备切换等功能，这一套系 统都在TDDL中完成。在外面有些系统也用DAL（数据访问层） 这个概念来命名这个中间件。
==========
